"""
AI Manager Module for Manim Video Generator.

This module provides integrations with different AI models for various stages
of the video generation process, including problem solving, animation planning,
script generation, and Manim code generation.
"""

import json
import asyncio
import hashlib
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential

from openai import OpenAI, AsyncOpenAI
from anthropic import Anthropic

from src.config.config import Config
from src.utils.logging_utils import get_logger
from src.core.animation_planner import AnimationPlan, AnimationPlanner

logger = get_logger(__name__)


class AnimationPlan(BaseModel):
    """Schema for the animation plan generated by GPT-4o."""
    title: str = Field(..., description="Title of the video")
    sections: List[Dict[str, Any]] = Field(..., description="List of sections in the animation")
    estimated_duration: int = Field(..., description="Estimated duration in seconds")
    visual_style: Dict[str, Any] = Field(..., description="Visual style guidelines")


class ResponseCache:
    """
    Cache for AI model responses to avoid redundant API calls.
    """
    
    def __init__(self, cache_dir: str = ".cache"):
        """
        Initialize the response cache.
        
        Args:
            cache_dir: Directory to store cache files
        """
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        logger.info(f"Initialized response cache in {cache_dir}")
    
    def _generate_cache_key(self, model: str, prompt: str, **kwargs) -> str:
        """
        Generate a unique cache key based on model, prompt, and other parameters.
        
        Args:
            model: The model name
            prompt: The prompt text
            **kwargs: Additional parameters that affect the response
            
        Returns:
            A unique hash string to use as cache key
        """
        # Create a string representation of all parameters
        param_str = f"{model}:{prompt}"
        for k, v in sorted(kwargs.items()):
            param_str += f":{k}={v}"
        
        # Generate a hash
        return hashlib.md5(param_str.encode()).hexdigest()
    
    def get_cached_response(self, model: str, prompt: str, **kwargs) -> Optional[str]:
        """
        Get a cached response if available.
        
        Args:
            model: The model name
            prompt: The prompt text
            **kwargs: Additional parameters that affect the response
            
        Returns:
            The cached response or None if not found
        """
        cache_key = self._generate_cache_key(model, prompt, **kwargs)
        cache_file = Path(self.cache_dir) / f"{cache_key}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f)
                logger.info(f"Cache hit for {model} query")
                return cached_data.get('response')
            except Exception as e:
                logger.error(f"Error reading cache file: {e}")
        
        logger.info(f"Cache miss for {model} query")
        return None
    
    def set_cached_response(self, model: str, prompt: str, response: str, **kwargs) -> None:
        """
        Cache a response for future use.
        
        Args:
            model: The model name
            prompt: The prompt text
            response: The response to cache
            **kwargs: Additional parameters that affect the response
        """
        cache_key = self._generate_cache_key(model, prompt, **kwargs)
        cache_file = Path(self.cache_dir) / f"{cache_key}.json"
        
        try:
            with open(cache_file, 'w') as f:
                json.dump({
                    'model': model,
                    'prompt': prompt,
                    'response': response,
                    'params': kwargs
                }, f)
            logger.info(f"Cached response for {model} query")
        except Exception as e:
            logger.error(f"Error writing to cache file: {e}")


class AIManager:
    """
    Manages interactions with various AI models for different stages of the video generation process.
    """
    
    def __init__(self, config: Config, use_cache: bool = True):
        """
        Initialize the AI manager.
        
        Args:
            config: Configuration object
            use_cache: Whether to use response caching
        """
        self.config = config
        self.use_cache = use_cache
        
        # Initialize clients
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.anthropic_client = Anthropic(api_key=config.ANTHROPIC_API_KEY)
        
        # Initialize cache if enabled
        self.response_cache = ResponseCache() if use_cache else None
        
        # Initialize animation planner
        self.animation_planner = AnimationPlanner()
        
        logger.info("AI Manager initialized")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def solve_math_problem(self, query: Dict[str, Any]) -> str:
        """
        Use OpenAI's o3-mini model to solve mathematical problems or explain theorems.
        
        Args:
            query: Dictionary containing the query and related information
            
        Returns:
            Comprehensive solution or explanation
        """
        if not self.openai_client:
            raise ValueError("OpenAI API key not provided")
            
        logger.info(f"Solving math problem: {query['query']}")
        
        # Construct the prompt
        prompt = f"""You are a world-class mathematician tasked with solving or explaining the following:
        
{query['query']}

If this is a problem to solve, provide a step-by-step solution showing all your work.
If this is a theorem or concept to explain, provide a clear explanation with definitions, importance, and examples.

Category: {query.get('category', 'not specified')}
Difficulty: {query.get('difficulty_level', 'not specified')}

Your response should be comprehensive, mathematically rigorous, and educational."""

        # Check cache first if enabled
        if self.use_cache and self.response_cache:
            cached_response = self.response_cache.get_cached_response("o3-mini", prompt)
            if cached_response:
                logger.info("Using cached o3-mini response")
                return cached_response

        # Call the OpenAI o3-mini model
        response = self.openai_client.chat.completions.create(
            model="o3-mini",
            messages=[
                {"role": "system", "content": "You are a brilliant mathematician who explains concepts clearly and solves problems with precision."},
                {"role": "user", "content": prompt}
            ]
        )
        
        explanation = response.choices[0].message.content
        logger.info("Math problem solved successfully")
        
        # Cache the response if enabled
        if self.use_cache and self.response_cache:
            self.response_cache.set_cached_response("o3-mini", prompt, explanation)
        
        return explanation
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def create_animation_plan(self, query: Dict[str, Any], explanation: str) -> AnimationPlan:
        """
        Use GPT-4o to create a structured animation plan for the video.
        
        Args:
            query: Dictionary containing the query and related information
            explanation: Mathematical explanation from o3-mini
            
        Returns:
            Structured animation plan
        """
        if not self.openai_client:
            raise ValueError("OpenAI API key not provided")
            
        logger.info(f"Creating animation plan for: {query['query']}")
        
        # Construct the prompt
        prompt = f"""You are an expert animation planner for mathematical videos. 
Your task is to create a detailed, structured plan for a Manim animation that explains the following:

QUERY: {query['query']}

MATHEMATICAL EXPLANATION:
{explanation}

Create a JSON plan for the animation with the following structure:
```json
{{
  "title": "Title of the Animation",
  "sections": [
    {{
      "id": "section1",
      "title": "Section Title",
      "duration": 30,
      "narration_summary": "What will be explained in this section",
      "visual_elements": [
        {{
          "type": "text/equation/shape/graph/etc",
          "content": "Specific content to display",
          "animation": "Type of animation to use (FadeIn, Transform, etc.)",
          "duration": 5,
          "sync_with_narration": "Text that should be spoken during this element"
        }}
      ]
    }}
  ],
  "estimated_duration": 180,
  "visual_style": {{
    "color_theme": "dark/light",
    "font_size": "medium",
    "background_color": "#1C1C1C",
    "accent_color": "#3B82F6"
  }}
}}
```

Each visual element should correspond to a specific part of the explanation, creating a cohesive educational experience.
Ensure precise timing specifications to enable proper synchronization with narration.
The plan should be comprehensive but focused, highlighting key concepts without overwhelming viewers.

Your response should ONLY contain the valid JSON plan, no additional explanation."""

        # Check cache first if enabled
        if self.use_cache and self.response_cache:
            cached_response = self.response_cache.get_cached_response("gpt-4o", prompt, category=query.get('category', 'concept'))
            if cached_response:
                logger.info("Using cached GPT-4o animation plan")
                plan = json.loads(cached_response)
                return self.animation_planner.enhance_plan(plan, explanation, query.get('category', 'concept'))

        # Call the OpenAI GPT-4o model
        response = self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert animation planner for mathematical educational videos."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2,
            max_tokens=4000
        )
        
        # Parse the JSON response
        plan_json = response.choices[0].message.content
        plan = json.loads(plan_json)
        
        # Validate with pydantic
        animation_plan = AnimationPlan(**plan)
        logger.info("Animation plan created successfully")
        
        # Cache the response if enabled
        if self.use_cache and self.response_cache:
            self.response_cache.set_cached_response("gpt-4o", prompt, plan_json, category=query.get('category', 'concept'))
        
        return self.animation_planner.enhance_plan(animation_plan, explanation, query.get('category', 'concept'))
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def generate_script(self, query: Dict[str, Any], explanation: str, animation_plan: AnimationPlan) -> Dict[str, str]:
        """
        Generate a narration script for the video using GPT-4o.
        
        Args:
            query: Dictionary containing the query and related information
            explanation: Mathematical explanation
            animation_plan: Structured animation plan
            
        Returns:
            Dictionary mapping section IDs to script text
        """
        if not self.openai_client:
            raise ValueError("OpenAI API key not provided")
            
        logger.info("Generating narration script")
        
        # Construct the prompt
        prompt = f"""You are an expert educational scriptwriter. 
Create a natural, engaging narration script for a mathematical video explaining:

QUERY: {query['query']}

The script should align with this animation plan:
{json.dumps(animation_plan.dict(), indent=2)}

For each section, write precise narration text that:
1. Clearly explains the mathematical concepts
2. Aligns perfectly with the visual elements described in the plan
3. Uses appropriate transitions between sections
4. Has natural, conversational language that's easy to follow
5. Includes clear verbal cues for when specific animations/visuals should appear

Format your response as a JSON object with section IDs as keys and complete narration text as values:
```json
{{
  "section1": "Complete narration text for section 1...",
  "section2": "Complete narration text for section 2...",
  ...
}}
```

Your narration should be timed to match the specified durations in the animation plan."""

        # Check cache first if enabled
        if self.use_cache and self.response_cache:
            cached_response = self.response_cache.get_cached_response("gpt-4o", prompt)
            if cached_response:
                logger.info("Using cached GPT-4o script")
                return json.loads(cached_response)

        # Call the OpenAI GPT-4o model
        response = self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert educational scriptwriter for mathematical videos."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3,
            max_tokens=4000
        )
        
        # Parse the JSON response
        script_json = response.choices[0].message.content
        script = json.loads(script_json)
        
        logger.info("Script generation completed successfully")
        
        # Cache the response if enabled
        if self.use_cache and self.response_cache:
            self.response_cache.set_cached_response("gpt-4o", prompt, script_json)
        
        return script
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=3, max=15))
    async def generate_manim_code(self, query: Dict[str, Any], animation_plan: AnimationPlan, scripts: Dict[str, str]) -> str:
        """
        Generate Manim code using Claude 3.7 to create the animations.
        
        Args:
            query: Dictionary containing the query and related information
            animation_plan: Structured animation plan
            scripts: Narration scripts for each section
            
        Returns:
            Manim Python code
        """
        if not self.anthropic_client:
            raise ValueError("Anthropic API key not provided")
            
        logger.info("Generating Manim code")
        
        # Construct the prompt
        prompt = f"""You are an expert Manim programmer creating educational mathematics videos.

Generate complete, executable Manim code for a video that explains:
QUERY: {query['query']}

Following this animation plan:
{json.dumps(animation_plan.dict(), indent=2)}

With these narration scripts for each section:
{json.dumps(scripts, indent=2)}

Requirements:
1. Use manim-voiceover for audio synchronization:
   - Import with: from manim_voiceover import VoiceoverScene
   - Create a subclass of VoiceoverScene
   - Use self.add_voiceover() methods for narration

2. Create a complete, self-contained file that runs without errors
   - Include ALL necessary imports
   - Define ALL needed classes and functions
   - Create a proper Scene class structure

3. Implement precise animations that match the plan:
   - Follow the exact visual elements specified
   - Match the timing in the animation plan
   - Synchronize visuals with the narration script

4. Add comments explaining complex parts of the code

5. Optimize for performance where possible

6. Use the specified visual style from the animation plan

Your code should be DIRECTLY RUNNABLE with manim and manim-voiceover libraries.
Focus on correctness and precise timing for audio-visual synchronization."""

        # Check cache first if enabled
        if self.use_cache and self.response_cache:
            cached_response = self.response_cache.get_cached_response("claude-3-7-sonnet-20240229", prompt)
            if cached_response:
                logger.info("Using cached Claude Manim code")
                return cached_response

        # Call the Anthropic Claude 3.7 model
        response = asyncio.to_thread(
            self.anthropic_client.messages.create,
            model="claude-3-7-sonnet-20240229",
            max_tokens=4000,
            temperature=0.2,
            system="You are an expert Manim programmer who creates high-quality, executable code for mathematical animations. Focus on writing clean, correct, and optimized code.",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # Extract the code from the response
        content = response.content[0].text
        
        # Find Python code blocks
        import re
        code_blocks = re.findall(r'```python\n(.*?)```', content, re.DOTALL)
        
        if code_blocks:
            manim_code = code_blocks[0]
        else:
            # If no code block is found, use the entire response (less ideal)
            manim_code = content
            
        logger.info("Manim code generation completed successfully")
        
        # Cache the response if enabled
        if self.use_cache and self.response_cache:
            self.response_cache.set_cached_response("claude-3-7-sonnet-20240229", prompt, manim_code)
        
        return manim_code
    
    async def generate_content_concurrently(self, query: Dict[str, Any], explanation: str, animation_plan: AnimationPlan) -> Tuple[Dict[str, str], str]:
        """
        Generate the script and Manim code concurrently for maximum performance.
        
        Args:
            query: Dictionary containing the query and related information
            explanation: Mathematical explanation
            animation_plan: Structured animation plan
            
        Returns:
            Tuple of (scripts, manim_code)
        """
        logger.info("Starting concurrent content generation")
        
        # Start both tasks concurrently
        script_task = asyncio.create_task(self.generate_script(query, explanation, animation_plan))
        
        # We need to wait for the script before generating Manim code
        scripts = await script_task
        
        # Generate Manim code with the scripts
        manim_code = await self.generate_manim_code(query, animation_plan, scripts)
        
        logger.info("Concurrent content generation completed")
        return scripts, manim_code
    
    def get_model_usage(self) -> Dict[str, Dict[str, int]]:
        """
        Get the current model usage statistics.
        
        Returns:
            Dictionary of model usage metrics
        """
        return {
            "tokens": {"o3-mini": 0, "gpt-4o": 0, "claude-3-7-sonnet-20240229": 0},
            "calls": {"o3-mini": 0, "gpt-4o": 0, "claude-3-7-sonnet-20240229": 0},
            "cache_hits": {"o3-mini": 0, "gpt-4o": 0, "claude-3-7-sonnet-20240229": 0}
        } 